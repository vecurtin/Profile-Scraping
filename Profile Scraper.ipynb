{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43af6b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in ./opt/anaconda3/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: cryptography>=2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (3.4.8)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: Twisted[http2]>=17.9.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (22.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: parsel>=1.5.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (21.0.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (0.4.0)\n",
      "Requirement already satisfied: h2<4.0,>=3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: lxml>=3.5.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (4.6.3)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in ./opt/anaconda3/lib/python3.9/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in ./opt/anaconda3/lib/python3.9/site-packages (from cryptography>=2.0->scrapy) (1.14.6)\n",
      "Requirement already satisfied: pycparser in ./opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from h2<4.0,>=3.0->scrapy) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from h2<4.0,>=3.0->scrapy) (3.0.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in ./opt/anaconda3/lib/python3.9/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in ./opt/anaconda3/lib/python3.9/site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (21.2.0)\n",
      "Requirement already satisfied: pyasn1 in ./opt/anaconda3/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in ./opt/anaconda3/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in ./opt/anaconda3/lib/python3.9/site-packages (from Twisted[http2]>=17.9.0->scrapy) (3.10.0.2)\n",
      "Requirement already satisfied: incremental>=21.3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from Twisted[http2]>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in ./opt/anaconda3/lib/python3.9/site-packages (from Twisted[http2]>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in ./opt/anaconda3/lib/python3.9/site-packages (from Twisted[http2]>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in ./opt/anaconda3/lib/python3.9/site-packages (from Twisted[http2]>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: priority<2.0,>=1.1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from Twisted[http2]>=17.9.0->scrapy) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (3.2)\n",
      "Requirement already satisfied: setuptools in ./opt/anaconda3/lib/python3.9/site-packages (from zope.interface>=4.1.3->scrapy) (58.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a9564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: insta-scrape in ./opt/anaconda3/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: requests in ./opt/anaconda3/lib/python3.9/site-packages (from insta-scrape) (2.26.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./opt/anaconda3/lib/python3.9/site-packages (from insta-scrape) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->insta-scrape) (2.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->insta-scrape) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->insta-scrape) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->insta-scrape) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->insta-scrape) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install insta-scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a92ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping profile \n",
    "#scraper94118 = fake profile /username \n",
    "\n",
    "#to scrape posts and hashtag \n",
    "#to_dict\n",
    "#to_json\n",
    "#to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d871371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from instascrape import Profile \n",
    "profile = Profile('scraper94118')\n",
    "profile.scrape()\n",
    "recent_posts = profile.get_recent_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de418fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulatising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be14d724",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recent_posts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wx/8zdpn2f91j7cfrn3yfr98qfc0000gn/T/ipykernel_23139/1005339164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mposts_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecent_posts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mposts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'upload_date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comments'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'likes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recent_posts' is not defined"
     ]
    }
   ],
   "source": [
    "posts_data = [post.to_dict() for post in recent_posts]\n",
    "posts_df = pd.DataFrame(posts_data)\n",
    "print(posts_df[['upload_date', 'comments', 'likes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d2fa7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wx/8zdpn2f91j7cfrn3yfr98qfc0000gn/T/ipykernel_23139/691034357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seaborn-darkgrid'\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Stylistic change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Plot the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Upload Date'\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# Write labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Likes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.style.use('seaborn-darkgrid')      # Stylistic change\n",
    "\n",
    "plt.scatter(df.upload_date, df.likes)  # Plot the data\n",
    "plt.xlabel('Upload Date')              # Write labels\n",
    "plt.ylabel('Likes')\n",
    "plt.title('@chris_greening Likes per Post')\n",
    "plt.show()                             # Show graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import datetime \n",
    "\n",
    "import instascrape\n",
    "\n",
    "def track_post(url: str):\n",
    "    \"\"\"\n",
    "    Return a list of datetimes and a list of strings from an \n",
    "    Instagram post scraped across a one hour period.\n",
    "    \"\"\" \n",
    "    times = [] \n",
    "    likes = []\n",
    "    now = datetime.datetime.now()\n",
    "    end_time = now + datetime.timedelta(hours=1)\n",
    "\n",
    "    while now < end_time:\n",
    "        time.sleep(60)     \n",
    "\n",
    "        post = instascrape.Post(url) \n",
    "        post.scrape()\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        times.append(now)\n",
    "        likes.append(post.likes)\n",
    "    return times, likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#location scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9edb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from instascrape import Location \n",
    "url = 'https://www.instagram.com/explore/locations/212988663/new-york-new-york/'\n",
    "new_york = Location(url)\n",
    "new_york.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The NY location tag has {new_york.amount_of_posts:,} posts\")\n",
    ">>> The NY location tag has 61,202,403 posts.\n",
    "\n",
    "print(f\"NY tag geographic coordinates: ({new_york.latitude}, {new_york.longitude}\")\n",
    ">>> NY tag geographic coordinates: (40.7142, -74.0064)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second method that i've foudn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject instascraper\n",
    "\n",
    "cd instascraper\n",
    "\n",
    "scrapy genspider instagram instagram.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get five functions from this \n",
    "#start_requests - will construct the Instagram URL for the users account and send the request to Instagram.\n",
    "#parse - will extract all the posts data from the users news feed.\n",
    "#parse_page - if there is more than one page, this function will parse all the posts data from those pages.\n",
    "#get_video - if the post includes a video, this function will be called and extract the videos url.\n",
    "#get_url - will send the request to Scraper API so it can retrieve the HTML response.\n",
    "\n",
    "#from what i've seen Scraper API doesn't work on instragram anymore so that's the above that may work better? ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3695da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile URL \n",
    "https://www.instagram.com/<user_name>/\n",
    "\n",
    "## Tags URL\n",
    "https://www.instagram.com/explore/tags/<example_tag>/\n",
    "\n",
    "## Location URL\n",
    "https://www.instagram.com/explore/locations/<location_id>/\n",
    "\n",
    "# Note: the location URL is a numeric value so you need to identify the location ID number for\n",
    "# the locations you want to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439578ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreiveing instragram accoutns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_requests(self):\n",
    "        for username in user_accounts:\n",
    "            url = f'https://www.instagram.com/{username}/?hl=en'\n",
    "            yield scrapy.Request(get_url(url), callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping posts JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "        x = response.xpath(\"//script[starts-with(.,'window._sharedData')]/text()\").extract_first()\n",
    "        json_string = x.strip().split('= ')[1][:-1]\n",
    "        data = json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "        x = response.xpath(\"//script[starts-with(.,'window._sharedData')]/text()\").extract_first()\n",
    "        json_string = x.strip().split('= ')[1][:-1]\n",
    "        data = json.loads(json_string)\n",
    "        # all that we have to do here is to parse the JSON we have\n",
    "        user_id = data['entry_data']['ProfilePage'][0]['graphql']['user']['id']\n",
    "        next_page_bool = \\\n",
    "            data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['page_info'][\n",
    "                'has_next_page']\n",
    "        edges = data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_felix_video_timeline']['edges']\n",
    "        for i in edges:\n",
    "            url = 'https://www.instagram.com/p/' + i['node']['shortcode']\n",
    "            video = i['node']['is_video']\n",
    "            date_posted_timestamp = i['node']['taken_at_timestamp']\n",
    "            date_posted_human = datetime.fromtimestamp(date_posted_timestamp).strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            like_count = i['node']['edge_liked_by']['count'] if \"edge_liked_by\" in i['node'].keys() else ''\n",
    "            comment_count = i['node']['edge_media_to_comment']['count'] if 'edge_media_to_comment' in i[\n",
    "                'node'].keys() else ''\n",
    "            captions = \"\"\n",
    "            if i['node']['edge_media_to_caption']:\n",
    "                for i2 in i['node']['edge_media_to_caption']['edges']:\n",
    "                    captions += i2['node']['text'] + \"\\n\"\n",
    "\n",
    "            if video:\n",
    "                image_url = i['node']['display_url']\n",
    "            else:\n",
    "                image_url = i['node']['thumbnail_resources'][-1]['src']\n",
    "            item = {'postURL': url, 'isVideo': video, 'date_posted': date_posted_human,\n",
    "                    'timestamp': date_posted_timestamp, 'likeCount': like_count, 'commentCount': comment_count, 'image_url': image_url,\n",
    "                    'captions': captions[:-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting video URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8dabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if video:\n",
    "     yield scrapy.Request(get_url(url), callback=self.get_video, meta={'item': item})\n",
    "else:\n",
    "     item['videoURL'] = ''\n",
    "     yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c74a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(self, response):\n",
    "        # only from the first page\n",
    "        item = response.meta['item']\n",
    "        video_url = response.xpath('//meta[@property=\"og:video\"]/@content').extract_first()\n",
    "        item['videoURL'] = video_url\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea963cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up proxies \n",
    "#API so not sure if it works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "API = ‘<YOUR_API_KEY>’\n",
    "\n",
    "def get_url(url):\n",
    "    payload = {'api_key': API, 'url': url}\n",
    "    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)\n",
    "    return proxy_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b43590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_requests(self):\n",
    "        for username in user_accounts:\n",
    "            url = f'https://www.instagram.com/{username}/?hl=en'\n",
    "            yield scrapy.Request(get_url(url), callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstagramSpider(scrapy.Spider):\n",
    "    name = 'instagram'\n",
    "    allowed_domains = ['api.scraperapi.com']\n",
    "    custom_settings = {'CONCURRENT_REQUESTS_PER_DOMAIN': 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52293650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up monitoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f52063",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scrapeops-scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c77260",
   "metadata": {},
   "outputs": [],
   "source": [
    "## settings.py\n",
    "\n",
    "## Add Your ScrapeOps API key\n",
    "SCRAPEOPS_API_KEY = 'YOUR_API_KEY'\n",
    "\n",
    "## Add In The ScrapeOps Extension\n",
    "EXTENSIONS = {\n",
    " 'scrapeops_scrapy.extension.ScrapeOpsMonitor': 500, \n",
    "}\n",
    "\n",
    "## Update The Download Middlewares\n",
    "DOWNLOADER_MIDDLEWARES = { \n",
    "'scrapeops_scrapy.middleware.retry.RetryMiddleware': 550, \n",
    "'scrapy.downloadermiddlewares.retry.RetryMiddleware': None, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#go live "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl instagram -o test.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
